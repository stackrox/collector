---
- name: Create Benchmark VMs
  hosts: localhost
  tags:
    - setup
  tasks:
    - name: Create VMs for each Collection Method
      include_role:
        name: create-all-vms
      vars:
        # ensure that all listed VMs are created
        test_platform: all
        vm_list:
          rhel: "{{ virtual_machines['rhel'] }}"
          ubuntu-os: "{{ virtual_machines['ubuntu-os'] }}"
        collection_method: "core_bpf"
  post_tasks:
    # We have all the VMs created now, so refresh the inventory - this allows
    # us to use the provisioning role on the VMs we just created
    - meta: refresh_inventory

- name: Provision Benchmark VMs
  hosts: "job_id_{{ job_id }}"
  roles:
    - provision-vm
  gather_facts: false
  strategy: free
  become: true
  tags:
    - setup
    - provision

- name: Run Benchmarks
  # using platform_*:&job_id here ensures that ansible imports
  # group_vars/platform_*.yml for the relevant platform, e.g.
  # flatcar where we need to use a custom python interpreter.
  hosts: "platform_*:&job_id_{{ job_id }}"
  strategy: free
  roles:
    - run-test-target
  vars:
    # This is going to be the prefix for tests to be run. Originally it was
    # just TestBenchmark, to include both TestBenchmarkBaseline and
    # TestBenchmarkCollector, but since now we do benchmarking directly there
    # is no need in baseline results.
    collector_test: TestBenchmarkCollector
  tags:
    - run-benchmarks

- name: Teardown Benchmark VMs
  hosts: "job_id_{{ job_id }}"
  strategy: free
  roles:
    - destroy-vm
  tags:
    - teardown
