diff --git a/bpf/Makefile b/bpf/Makefile
index 5c745fc..f946d4d 100644
--- a/bpf/Makefile
+++ b/bpf/Makefile
@@ -5,6 +5,8 @@
 # MIT.txt or GPL.txt for full copies of the license.
 #
 
+always-y += probe.o
+# kept for compatibility with kernels < 5.11
 always += probe.o
 
 LLC ?= llc
@@ -18,6 +20,8 @@ KERNELDIR ?= /lib/modules/$(shell uname -r)/build
 ifeq ($(shell grep -sq "^\s*struct\s\+audit_task_info\s\+\*audit;\s*$$" $(KERNELDIR)/include/linux/sched.h ; echo $$? ), 0)
 	KBUILD_CPPFLAGS+= -DCOS_73_WORKAROUND
 endif
+# clang-7 does not support -fmacro-prefix-map
+KBUILD_CPPFLAGS:=$(filter-out -fmacro-prefix-map=%,$(KBUILD_CPPFLAGS))
 # End StackRox
 
 all:
diff --git a/bpf/filler_helpers.h b/bpf/filler_helpers.h
index 1a15376..2265725 100644
--- a/bpf/filler_helpers.h
+++ b/bpf/filler_helpers.h
@@ -215,16 +215,16 @@ static __always_inline bool bpf_getsockname(struct socket *sock,
 static __always_inline int bpf_addr_to_kernel(void *uaddr, int ulen,
 					      struct sockaddr *kaddr)
 {
-	if (ulen < 0 || ulen > sizeof(struct sockaddr_storage))
+	int len = _READ(ulen);
+	if (len < 0 || len > sizeof(struct sockaddr_storage))
 		return -EINVAL;
-
-	if (ulen == 0)
+	if (len == 0)
 		return 0;
 
 #ifdef BPF_FORBIDS_ZERO_ACCESS
-	if (bpf_probe_read(kaddr, ((ulen - 1) & 0xff) + 1, uaddr))
+	if (bpf_probe_read(kaddr, ((len - 1) & 0xff) + 1, uaddr))
 #else
-	if (bpf_probe_read(kaddr, ulen & 0xff, uaddr))
+	if (bpf_probe_read(kaddr, len & 0xff, uaddr))
 #endif
 		return -EFAULT;
 
@@ -728,17 +728,20 @@ static __always_inline int __bpf_val_to_ring(struct filler_data *data,
 {
 	unsigned int len_dyn = 0;
 	unsigned int len;
+	unsigned long curoff_bounded;
 
+	curoff_bounded = data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF;
 	if (data->state->tail_ctx.curoff > SCRATCH_SIZE_HALF)
 		return PPM_FAILURE_BUFFER_FULL;
 
 	if (dyn_idx != (u8)-1) {
-		*((u8 *)&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF]) = dyn_idx;
+		*((u8 *)&data->buf[curoff_bounded]) = dyn_idx;
 		len_dyn = sizeof(u8);
 		data->state->tail_ctx.curoff += len_dyn;
 		data->state->tail_ctx.len += len_dyn;
 	}
 
+	curoff_bounded = data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF;
 	if (data->state->tail_ctx.curoff > SCRATCH_SIZE_HALF)
 		return PPM_FAILURE_BUFFER_FULL;
 
@@ -748,7 +751,7 @@ static __always_inline int __bpf_val_to_ring(struct filler_data *data,
 		if (!data->curarg_already_on_frame) {
 			int res;
 
-			res = bpf_probe_read_str(&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF],
+			res = bpf_probe_read_str(&data->buf[curoff_bounded],
 						 PPM_MAX_ARG_SIZE,
 						 (const void *)val);
 			if (res <= 0)
@@ -771,15 +774,15 @@ static __always_inline int __bpf_val_to_ring(struct filler_data *data,
 					dpi_lookahead_size = len;
 
 				if (!data->curarg_already_on_frame) {
-					volatile unsigned long read_size = dpi_lookahead_size;
+					volatile u16 read_size = dpi_lookahead_size;
 
 #ifdef BPF_FORBIDS_ZERO_ACCESS
 					if (read_size)
-						if (bpf_probe_read(&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF],
+						if (bpf_probe_read(&data->buf[curoff_bounded],
 								   ((read_size - 1) & SCRATCH_SIZE_HALF) + 1,
 								   (void *)val))
 #else
-					if (bpf_probe_read(&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF],
+					if (bpf_probe_read(&data->buf[curoff_bounded],
 							   read_size & SCRATCH_SIZE_HALF,
 							   (void *)val))
 #endif
@@ -796,15 +799,15 @@ static __always_inline int __bpf_val_to_ring(struct filler_data *data,
 				len = PPM_MAX_ARG_SIZE;
 
 			if (!data->curarg_already_on_frame) {
-				volatile unsigned long read_size = len;
+				volatile u16 read_size = len;
 
 #ifdef BPF_FORBIDS_ZERO_ACCESS
 				if (read_size)
-					if (bpf_probe_read(&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF],
+					if (bpf_probe_read(&data->buf[curoff_bounded],
 							   ((read_size - 1) & SCRATCH_SIZE_HALF) + 1,
 							   (void *)val))
 #else
-				if (bpf_probe_read(&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF],
+				if (bpf_probe_read(&data->buf[curoff_bounded],
 						   read_size & SCRATCH_SIZE_HALF,
 						   (void *)val))
 #endif
@@ -830,13 +833,13 @@ static __always_inline int __bpf_val_to_ring(struct filler_data *data,
 	case PT_FLAGS8:
 	case PT_UINT8:
 	case PT_SIGTYPE:
-		*((u8 *)&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF]) = val;
+		*((u8 *)&data->buf[curoff_bounded]) = val;
 		len = sizeof(u8);
 		break;
 	case PT_FLAGS16:
 	case PT_UINT16:
 	case PT_SYSCALLID:
-		*((u16 *)&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF]) = val;
+		*((u16 *)&data->buf[curoff_bounded]) = val;
 		len = sizeof(u16);
 		break;
 	case PT_FLAGS32:
@@ -845,32 +848,32 @@ static __always_inline int __bpf_val_to_ring(struct filler_data *data,
 	case PT_UID:
 	case PT_GID:
 	case PT_SIGSET:
-		*((u32 *)&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF]) = val;
+		*((u32 *)&data->buf[curoff_bounded]) = val;
 		len = sizeof(u32);
 		break;
 	case PT_RELTIME:
 	case PT_ABSTIME:
 	case PT_UINT64:
-		*((u64 *)&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF]) = val;
+		*((u64 *)&data->buf[curoff_bounded]) = val;
 		len = sizeof(u64);
 		break;
 	case PT_INT8:
-		*((s8 *)&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF]) = val;
+		*((s8 *)&data->buf[curoff_bounded]) = val;
 		len = sizeof(s8);
 		break;
 	case PT_INT16:
-		*((s16 *)&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF]) = val;
+		*((s16 *)&data->buf[curoff_bounded]) = val;
 		len = sizeof(s16);
 		break;
 	case PT_INT32:
-		*((s32 *)&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF]) = val;
+		*((s32 *)&data->buf[curoff_bounded]) = val;
 		len = sizeof(s32);
 		break;
 	case PT_INT64:
 	case PT_ERRNO:
 	case PT_FD:
 	case PT_PID:
-		*((s64 *)&data->buf[data->state->tail_ctx.curoff & SCRATCH_SIZE_HALF]) = val;
+		*((s64 *)&data->buf[curoff_bounded]) = val;
 		len = sizeof(s64);
 		break;
 	default: {
diff --git a/bpf/fillers.h b/bpf/fillers.h
index b687e28..69669d4 100644
--- a/bpf/fillers.h
+++ b/bpf/fillers.h
@@ -20,6 +20,7 @@ or GPL2.txt for full copies of the license.
 //#define COS_73_WORKAROUND
 
 #include "../ppm_flag_helpers.h"
+#include "../kernel_hacks.h"
 
 #include <linux/tty.h>
 #include <linux/audit.h>
@@ -274,28 +275,28 @@ static __always_inline int bpf_poll_parse_fds(struct filler_data *data,
 #endif
 		return PPM_FAILURE_INVALID_USER_MEMORY;
 
+	if (data->state->tail_ctx.curoff > SCRATCH_SIZE_HALF)
+		return PPM_FAILURE_BUFFER_FULL;
+
 	off = data->state->tail_ctx.curoff + sizeof(u16);
 	fds_count = 0;
 
 	#pragma unroll
 	for (j = 0; j < POLL_MAXFDS; ++j) {
-		u16 flags;
+		if (off > SCRATCH_SIZE_HALF)
+			return PPM_FAILURE_BUFFER_FULL;
 
 		if (j == nfds)
 			break;
 
+		u16 flags;
+
 		if (enter_event) {
 			flags = poll_events_to_scap(fds[j].events);
 		} else {
-			if (!fds[j].revents)
-				continue;
-
 			flags = poll_events_to_scap(fds[j].revents);
 		}
 
-		if (off > SCRATCH_SIZE_HALF)
-			return PPM_FAILURE_BUFFER_FULL;
-
 		*(s64 *)&data->buf[off & SCRATCH_SIZE_HALF] = fds[j].fd;
 		off += sizeof(s64);
 
@@ -391,6 +392,10 @@ static __always_inline int bpf_parse_readv_writev_bufs(struct filler_data *data,
 		if (j == iovcnt)
 			break;
 
+		// BPF seems to require a hard limit to avoid overflows
+		if (size == LONG_MAX)
+			break;
+
 		size += iov[j].iov_len;
 	}
 
@@ -407,6 +412,7 @@ static __always_inline int bpf_parse_readv_writev_bufs(struct filler_data *data,
 	if (flags & PRB_FLAG_PUSH_DATA) {
 		if (size > 0) {
 			unsigned long off = data->state->tail_ctx.curoff;
+			unsigned long off_bounded;
 			unsigned long remaining = size;
 			int j;
 
@@ -417,6 +423,7 @@ static __always_inline int bpf_parse_readv_writev_bufs(struct filler_data *data,
 				if (j == iovcnt)
 					break;
 
+				off_bounded = off & SCRATCH_SIZE_HALF;
 				if (off > SCRATCH_SIZE_HALF)
 					break;
 
@@ -430,11 +437,11 @@ static __always_inline int bpf_parse_readv_writev_bufs(struct filler_data *data,
 
 #ifdef BPF_FORBIDS_ZERO_ACCESS
 				if (to_read)
-					if (bpf_probe_read(&data->buf[off & SCRATCH_SIZE_HALF],
+					if (bpf_probe_read(&data->buf[off_bounded],
 							   ((to_read - 1) & SCRATCH_SIZE_HALF) + 1,
 							   iov[j].iov_base))
 #else
-				if (bpf_probe_read(&data->buf[off & SCRATCH_SIZE_HALF],
+				if (bpf_probe_read(&data->buf[off_bounded],
 						   to_read & SCRATCH_SIZE_HALF,
 						   iov[j].iov_base))
 #endif
@@ -1538,11 +1545,13 @@ static __always_inline int __bpf_append_cgroup(struct css_set *cgroups,
 #endif
 	char *cgroup_path[MAX_CGROUP_PATHS];
 	int off = *len;
+	unsigned int off_bounded;
 
+	off_bounded = off & SCRATCH_SIZE_HALF;
 	if (off > SCRATCH_SIZE_HALF)
 		return PPM_FAILURE_BUFFER_FULL;
 
-	int res = bpf_probe_read_str(&buf[off & SCRATCH_SIZE_HALF],
+	int res = bpf_probe_read_str(&buf[off_bounded],
 				     SCRATCH_SIZE_HALF,
 				     subsys_name);
 	if (res <= 0)
@@ -1550,11 +1559,13 @@ static __always_inline int __bpf_append_cgroup(struct css_set *cgroups,
 
 	off += res - 1;
 
+	off_bounded = off & SCRATCH_SIZE_HALF;
 	if (off > SCRATCH_SIZE_HALF)
 		return PPM_FAILURE_BUFFER_FULL;
 
-	buf[off & SCRATCH_SIZE_HALF] = '=';
+	buf[off_bounded] = '=';
 	++off;
+	off_bounded = off & SCRATCH_SIZE_HALF;
 
 	#pragma unroll MAX_CGROUP_PATHS
 	for (int k = 0; k < MAX_CGROUP_PATHS; ++k) {
@@ -1582,14 +1593,15 @@ static __always_inline int __bpf_append_cgroup(struct css_set *cgroups,
 			if (off > SCRATCH_SIZE_HALF)
 				return PPM_FAILURE_BUFFER_FULL;
 
-			buf[off & SCRATCH_SIZE_HALF] = '/';
+			buf[off_bounded] = '/';
 			++off;
+			off_bounded = off & SCRATCH_SIZE_HALF;
 		}
 
 		if (off > SCRATCH_SIZE_HALF)
 			return PPM_FAILURE_BUFFER_FULL;
 
-		res = bpf_probe_read_str(&buf[off & SCRATCH_SIZE_HALF],
+		res = bpf_probe_read_str(&buf[off_bounded],
 						SCRATCH_SIZE_HALF,
 						cgroup_path[k]);
 		if (res <= 0)
@@ -1597,17 +1609,18 @@ static __always_inline int __bpf_append_cgroup(struct css_set *cgroups,
 
 		if (res > 1) {
 			if (res == sizeof("/") &&
-			    buf[off & SCRATCH_SIZE_HALF] == '/')
+			    buf[off_bounded] == '/')
 				--res;
 
 			off += res - 1;
+			off_bounded = off & SCRATCH_SIZE_HALF;
 		}
 	}
 
 	if (off > SCRATCH_SIZE_HALF)
 		return PPM_FAILURE_BUFFER_FULL;
 
-	buf[off & SCRATCH_SIZE_HALF] = 0;
+	buf[off_bounded] = 0;
 	++off;
 	*len = off;
 
diff --git a/bpf/ring_helpers.h b/bpf/ring_helpers.h
index a296af7..90850bc 100644
--- a/bpf/ring_helpers.h
+++ b/bpf/ring_helpers.h
@@ -34,6 +34,10 @@ static __always_inline void fixup_evt_arg_len(char *p,
 					      unsigned int argnum,
 					      unsigned int arglen)
 {
+	if (argnum > PPM_MAX_EVENT_PARAMS)
+	{
+		return;
+	}
 	volatile unsigned int argnumv = argnum;
 	*((u16 *)&p[sizeof(struct ppm_evt_hdr)] + (argnumv & (PPM_MAX_EVENT_PARAMS - 1))) = arglen;
 }
diff --git a/kernel_hacks.h b/kernel_hacks.h
new file mode 100644
index 0000000..e76f909
--- /dev/null
+++ b/kernel_hacks.h
@@ -0,0 +1,39 @@
+/*
+  * Copyright (c) 2020 Draios Inc. dba Sysdig.
+  *
+  * This file is dual licensed under either the MIT or GPL 2. See MIT.txt
+  * or GPL2.txt for full copies of the license.
+  */
+
+/**
+ * @file kernel_hacks.h
+ *
+ * This file contains kernel-version-dependent preprocessor instructions to
+ * help the driver compile on as many kernel versions as possible.
+ */
+
+#include <linux/version.h>
+
+/*
+ * Linux 5.6 kernels no longer include the old 32-bit timeval
+ * structures. But the syscalls (might) still use them.
+ */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 6, 0)
+#include <linux/time64.h>
+ struct compat_timespec {
+ 	int32_t tv_sec;
+ 	int32_t tv_nsec;
+ };
+
+ struct timespec {
+ 	int32_t tv_sec;
+ 	int32_t tv_nsec;
+ };
+
+ struct timeval {
+ 	int32_t tv_sec;
+ 	int32_t tv_usec;
+ };
+#else
+#define timeval64 timeval
+#endif
diff --git a/main.c b/main.c
index 8ba4c08..a91cb97 100644
--- a/main.c
+++ b/main.c
@@ -123,7 +123,7 @@ static int ppm_mmap(struct file *filp, struct vm_area_struct *vma);
 static int record_event_consumer(struct ppm_consumer_t *consumer,
 	enum ppm_event_type event_type,
 	enum syscall_flags drop_flags,
-	struct timespec *ts,
+	nanoseconds ns,
 	struct event_data_t *event_datap);
 static void record_event_all_consumers(enum ppm_event_type event_type,
 	enum syscall_flags drop_flags,
@@ -140,7 +140,7 @@ static int record_event_consumer_for(struct task_struct *task,
 	struct ppm_consumer_t *consumer,
 	enum ppm_event_type event_type,
 	enum syscall_flags drop_flags,
-	struct timespec *ts,
+	nanoseconds ns,
 	struct event_data_t *event_datap);
 
 static void record_event_all_consumers_for(struct task_struct* task,
@@ -256,6 +256,11 @@ do {								\
 		pr_info(fmt, ##__VA_ARGS__);			\
 } while (0)
 
+static inline nanoseconds ppm_nsecs(void)
+{
+	return ktime_to_ns(ktime_get_real());
+}
+
 inline void ppm_syscall_get_arguments(struct task_struct *task, struct pt_regs *regs, unsigned long *args)
 {
 #if (LINUX_VERSION_CODE < KERNEL_VERSION(5, 1, 0))
@@ -1191,7 +1196,6 @@ cleanup_ioctl_procinfo:
 	case PPM_IOCTL_DISABLE_DROPPING_MODE:
 	{
 		struct event_data_t event_data;
-		struct timespec ts;
 
 		vpr_info("PPM_IOCTL_DISABLE_DROPPING_MODE, consumer %p\n", consumer_id);
 
@@ -1203,12 +1207,11 @@ cleanup_ioctl_procinfo:
 		 * Push an event into the ring buffer so that the user can know that dropping
 		 * mode has been disabled
 		 */
-		getnstimeofday(&ts);
 		event_data.category = PPMC_CONTEXT_SWITCH;
 		event_data.event_info.context_data.sched_prev = (void *)DEI_DISABLE_DROPPING;
 		event_data.event_info.context_data.sched_next = (void *)0;
 
-		record_event_consumer(consumer, PPME_SYSDIGEVENT_E, UF_NEVER_DROP, &ts, &event_data);
+		record_event_consumer(consumer, PPME_SYSDIGEVENT_E, UF_NEVER_DROP, ppm_nsecs(), &event_data);
 
 		ret = 0;
 		goto cleanup_ioctl;
@@ -1770,11 +1773,11 @@ static enum ppm_event_type parse_socketcall_for(struct task_struct* task, struct
 #endif /* _HAS_SOCKETCALL */
 
 /* Begin StackRox Section */
-static inline void record_drop_e_for(struct task_struct* task, struct ppm_consumer_t *consumer, struct timespec *ts)
+static inline void record_drop_e_for(struct task_struct* task, struct ppm_consumer_t *consumer, nanoseconds ns)
 {
 	struct event_data_t event_data = {0};
 
-	if (record_event_consumer_for(task, consumer, PPME_DROP_E, UF_NEVER_DROP, ts, &event_data) == 0) {
+	if (record_event_consumer_for(task, consumer, PPME_DROP_E, UF_NEVER_DROP, ns, &event_data) == 0) {
 		consumer->need_to_insert_drop_e = 1;
 	} else {
 		if (consumer->need_to_insert_drop_e == 1)
@@ -1784,11 +1787,11 @@ static inline void record_drop_e_for(struct task_struct* task, struct ppm_consum
 	}
 }
 
-static inline void record_drop_x_for(struct task_struct* task, struct ppm_consumer_t *consumer, struct timespec *ts)
+static inline void record_drop_x_for(struct task_struct* task, struct ppm_consumer_t *consumer, nanoseconds ns)
 {
 	struct event_data_t event_data = {0};
 
-	if (record_event_consumer_for(task, consumer, PPME_DROP_X, UF_NEVER_DROP, ts, &event_data) == 0) {
+	if (record_event_consumer_for(task, consumer, PPME_DROP_X, UF_NEVER_DROP, ns, &event_data) == 0) {
 		consumer->need_to_insert_drop_x = 1;
 	} else {
 		if (consumer->need_to_insert_drop_x == 1)
@@ -1866,7 +1869,7 @@ static inline int drop_event_for(struct task_struct* task,
 			     struct ppm_consumer_t *consumer,
 			     enum ppm_event_type event_type,
 			     enum syscall_flags drop_flags,
-			     struct timespec *ts,
+			     nanoseconds ns,
 			     struct pt_regs *regs)
 {
 	int maybe_ret = 0;
@@ -1888,10 +1891,11 @@ static inline int drop_event_for(struct task_struct* task,
 			return 1;
 		}
 
-		if (ts->tv_nsec >= consumer->sampling_interval) {
+		if (consumer->sampling_interval < SECOND_IN_NS &&
+		    (ns % SECOND_IN_NS) >= consumer->sampling_interval) {
 			if (consumer->is_dropping == 0) {
 				consumer->is_dropping = 1;
-				record_drop_e_for(task, consumer, ts);
+				record_drop_e_for(task, consumer, ns);
 			}
 
 			return 1;
@@ -1899,7 +1903,7 @@ static inline int drop_event_for(struct task_struct* task,
 
 		if (consumer->is_dropping == 1) {
 			consumer->is_dropping = 0;
-			record_drop_x_for(task, consumer, ts);
+			record_drop_x_for(task, consumer, ns);
 		}
 	}
 
@@ -1911,18 +1915,16 @@ static void record_event_all_consumers_for(struct task_struct* task, enum ppm_ev
 	struct event_data_t *event_datap)
 {
 	struct ppm_consumer_t *consumer;
-	struct timespec ts;
+	nanoseconds ns = ppm_nsecs();
 
 	/* Begin StackRox section */
 	/* Moved this from record_event_consumers_for */
 	if (!test_bit(event_type, g_events_mask)) return;
 	/* End StackRox section */
 
-	getnstimeofday(&ts);
-
 	rcu_read_lock();
 	list_for_each_entry_rcu(consumer, &g_consumer_list, node) {
-		record_event_consumer_for(task, consumer, event_type, drop_flags, &ts, event_datap);
+		record_event_consumer_for(task, consumer, event_type, drop_flags, ns, event_datap);
 	}
 	rcu_read_unlock();
 }
@@ -1941,7 +1943,7 @@ static int record_event_consumer_for(struct task_struct* task,
 	struct ppm_consumer_t *consumer,
 	enum ppm_event_type event_type,
 	enum syscall_flags drop_flags,
-	struct timespec *ts,
+	nanoseconds ns,
 	struct event_data_t *event_datap)
 {
 	int res = 0;
@@ -1974,11 +1976,11 @@ static int record_event_consumer_for(struct task_struct* task,
 
 	if (event_type != PPME_DROP_E && event_type != PPME_DROP_X) {
 		if (consumer->need_to_insert_drop_e == 1)
-			record_drop_e_for(task, consumer, ts);
+			record_drop_e_for(task, consumer, ns);
 		else if (consumer->need_to_insert_drop_x == 1)
-			record_drop_x_for(task, consumer, ts);
+			record_drop_x_for(task, consumer, ns);
 
-		if (drop_event_for(task, consumer, event_type, drop_flags, ts,
+		if (drop_event_for(task, consumer, event_type, drop_flags, ns,
 			       event_datap->event_info.syscall_data.regs))
 			return res;
 	}
@@ -2102,7 +2104,7 @@ static int record_event_consumer_for(struct task_struct* task,
 #ifdef PPM_ENABLE_SENTINEL
 		hdr->sentinel_begin = ring->nevents;
 #endif
-		hdr->ts = timespec_to_ns(ts);
+		hdr->ts = ns;
 		hdr->tid = task->pid;
 		hdr->type = event_type;
 		hdr->nparams = args.nargs;
@@ -2252,7 +2254,7 @@ static int record_event_consumer_for(struct task_struct* task,
 		}
 	}
 
-	if (ts->tv_sec > ring->last_print_time.tv_sec + 1) {
+	if (MORE_THAN_ONE_SECOND_AHEAD(ns, ring->last_print_time)) {
 		vpr_info("consumer:%p CPU:%d, use:%d%%, ev:%llu, dr_buf:%llu, dr_pf:%llu, pr:%llu, cs:%llu\n",
 			   consumer->consumer_id,
 		       smp_processor_id(),
@@ -2263,7 +2265,7 @@ static int record_event_consumer_for(struct task_struct* task,
 		       ring_info->n_preemptions,
 		       ring->info->n_context_switches);
 
-		ring->last_print_time = *ts;
+		ring->last_print_time = ns;
 	}
 
 	atomic_dec(&ring->preempt_count);
@@ -2275,10 +2277,10 @@ static int record_event_consumer_for(struct task_struct* task,
 static int record_event_consumer(struct ppm_consumer_t *consumer,
 	enum ppm_event_type event_type,
 	enum syscall_flags drop_flags,
-	struct timespec *ts,
+	nanoseconds ns,
 	struct event_data_t *event_datap)
 {
-	return record_event_consumer_for(current, consumer, event_type, drop_flags, ts, event_datap);
+	return record_event_consumer_for(current, consumer, event_type, drop_flags, ns, event_datap);
 }
 
 /* End StackRox Section */
@@ -2657,7 +2659,7 @@ static void reset_ring_buffer(struct ppm_ring_buffer_context *ring)
 	ring->info->n_drops_pf = 0;
 	ring->info->n_preemptions = 0;
 	ring->info->n_context_switches = 0;
-	getnstimeofday(&ring->last_print_time);
+	ring->last_print_time = ppm_nsecs();
 }
 
 #if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 15, 0))
diff --git a/ppm.h b/ppm.h
index 27e5f2d..3054be8 100644
--- a/ppm.h
+++ b/ppm.h
@@ -21,7 +21,7 @@ or GPL2.txt for full copies of the license.
 #define ASSERT(expr)
 #endif
 
-#include <linux/time.h>
+typedef u64 nanoseconds;
 
 /* Begin StackRox Section */
 #include <linux/pid_namespace.h>
@@ -54,7 +54,7 @@ struct ppm_ring_buffer_context {
 	bool capture_enabled;
 	struct ppm_ring_buffer_info *info;
 	char *buffer;
-	struct timespec last_print_time;
+	nanoseconds last_print_time;
 	u32 nevents;
 	atomic_t preempt_count;
 	char *str_storage;	/* String storage. Size is one page. */
@@ -123,4 +123,7 @@ extern const enum ppm_syscall_code g_syscall_ia32_code_routing_table[];
 
 extern void ppm_syscall_get_arguments(struct task_struct *task, struct pt_regs *regs, unsigned long *args);
 
+#define SECOND_IN_NS 1000000000
+#define NS_TO_SEC(_ns) ((_ns) / SECOND_IN_NS)
+#define MORE_THAN_ONE_SECOND_AHEAD(_ns1, _ns2) ((_ns1) - (_ns2) > SECOND_IN_NS)
 #endif /* PPM_H_ */
diff --git a/ppm_fillers.c b/ppm_fillers.c
index 366cae8..919cd2a 100644
--- a/ppm_fillers.c
+++ b/ppm_fillers.c
@@ -47,6 +47,8 @@ or GPL2.txt for full copies of the license.
 #include <linux/bpf.h>
 #endif
 
+#include "kernel_hacks.h"
+
 #if LINUX_VERSION_CODE < KERNEL_VERSION(3, 9, 0)
 static inline struct inode *file_inode(struct file *f)
 {
@@ -627,9 +629,9 @@ static int compat_accumulate_argv_or_env(compat_uptr_t argv,
 
 #endif
 
-// probe_kernel_read() only added in kernel 2.6.26
+/* probe_kernel_read() only added in kernel 2.6.26, name changed in 5.8.0 */
 #if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 26)
-long probe_kernel_read(void *dst, const void *src, size_t size)
+long copy_from_kernel_nofault(void *dst, const void *src, size_t size)
 {
 	long ret;
 	mm_segment_t old_fs = get_fs();
@@ -642,8 +644,11 @@ long probe_kernel_read(void *dst, const void *src, size_t size)
 
 	return ret ? -EFAULT : 0;
 }
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
+#define copy_from_kernel_nofault probe_kernel_read
 #endif
 
+
 static int ppm_get_tty(void)
 {
 	/* Locking of the signal structures seems too complicated across
@@ -664,25 +669,25 @@ static int ppm_get_tty(void)
 	if (!sig)
 		return 0;
 
-	if (unlikely(probe_kernel_read(&tty, &sig->tty, sizeof(tty))))
+	if (unlikely(copy_from_kernel_nofault(&tty, &sig->tty, sizeof(tty))))
 		return 0;
 
 	if (!tty)
 		return 0;
 
-	if (unlikely(probe_kernel_read(&index, &tty->index, sizeof(index))))
+	if (unlikely(copy_from_kernel_nofault(&index, &tty->index, sizeof(index))))
 		return 0;
 
-	if (unlikely(probe_kernel_read(&driver, &tty->driver, sizeof(driver))))
+	if (unlikely(copy_from_kernel_nofault(&driver, &tty->driver, sizeof(driver))))
 		return 0;
 
 	if (!driver)
 		return 0;
 
-	if (unlikely(probe_kernel_read(&major, &driver->major, sizeof(major))))
+	if (unlikely(copy_from_kernel_nofault(&major, &driver->major, sizeof(major))))
 		return 0;
 
-	if (unlikely(probe_kernel_read(&minor_start, &driver->minor_start, sizeof(minor_start))))
+	if (unlikely(copy_from_kernel_nofault(&minor_start, &driver->minor_start, sizeof(minor_start))))
 		return 0;
 
 	tty_nr = new_encode_dev(MKDEV(major, minor_start) + index);
@@ -1347,6 +1352,7 @@ static int parse_sockopt(struct event_filler_arguments *args, int level, int opt
 		uint64_t val64;
 		struct timeval tv;
 	} u;
+	nanoseconds ns = 0;
 
 	if (level == SOL_SOCKET) {
 		switch (optname) {
@@ -1363,9 +1369,12 @@ static int parse_sockopt(struct event_filler_arguments *args, int level, int opt
 #ifdef SO_SNDTIMEO
 			case SO_SNDTIMEO:
 #endif
-				if (unlikely(ppm_copy_from_user(&u.tv, optval, sizeof(u.tv))))
+				if (unlikely(ppm_copy_from_user(&u.tv, optval, sizeof(u.tv)))) {
 					return PPM_FAILURE_INVALID_USER_MEMORY;
-				return val_to_ring(args, u.tv.tv_sec * 1000000000 + u.tv.tv_usec * 1000, 0, false, PPM_SOCKOPT_IDX_TIMEVAL);
+				}
+				ns = u.tv.tv_sec * SECOND_IN_NS + u.tv.tv_usec * 1000;
+				return val_to_ring(args, ns, 0, false, PPM_SOCKOPT_IDX_TIMEVAL);
+
 
 #ifdef SO_COOKIE
 			case SO_COOKIE:
@@ -2707,7 +2716,7 @@ static int timespec_parse(struct event_filler_arguments *args, unsigned long val
 #ifdef CONFIG_COMPAT
 	if (!args->compat) {
 #endif
-		cfulen = (int)ppm_copy_from_user(targetbuf, (void __user *)val, sizeof(struct timespec));
+		cfulen = (int)ppm_copy_from_user(targetbuf, (void __user *)val, sizeof(*tts));
 		if (unlikely(cfulen != 0))
 			return PPM_FAILURE_INVALID_USER_MEMORY;
 
